name: covid19ro
# A title for display in the UI:
description: "Covid 19 Wob Rijksoverheid"


pipeline:
  init:
    # This first stage will get the ball rolling with a seed URL.
    method: seed
    params:
      urls:
        - https://www.rijksoverheid.nl/documenten?trefwoord=Corona&startdatum=&einddatum=01-01-2021&onderdeel=Alle+ministeries&type=Wob-verzoek
    handle:
      pass: fetch

  fetch:
    # Download the seed page
    method: fetch
    params:
      # These rules specify which pages should be scraped or included:
      rules:
        and:
          - domain: www.rijksoverheid.nl
          - pattern: "https://www.rijksoverheid.nl/(binaries|documenten).*"
          - not:
              or:
                - mime_group: assets
                - mime_group: images
    handle:
      pass: parse

  parse:
    # Parse the scraped pages to find if they contain additional links.
    method: parse
    params:
      include_paths:
        - './/div[contains(@class, "results")]'
        - './/div[contains(@class, "paging-menu")]'
        - './/div[contains(@class, "download")]'
      meta:
        title: './/h1'
        description: './/div[contains(@class, "intro")]'
      # Additional rules to determine if a scraped page should be stored or not.
      # In this example, we're only keeping PDFs, word files, etc.
      store:
        or:
          - mime_group: web
          - mime_group: archives
          - mime_group: documents
          - pattern: "https://www.rijksoverheid.nl/(binaries|documenten)/.*"

    handle:
      store: shorten
      # this makes it a recursive web crawler:
      fetch: fetch

  shorten:
    method: memorious_utils.file:shorten
    params:
      max_length: 150
    handle:
      pass: store

  store:
    # Store the crawled documents to a directory
    method: directory
    params:
      path: /data/results
